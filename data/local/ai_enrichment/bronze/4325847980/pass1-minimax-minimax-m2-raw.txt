<reasoning>I need to carefully extract information from this IBM job posting for a Data Engineer position in Hyderabad, India. Let me go through each section systematically.

First, let me identify what information is explicitly stated vs. what needs to be inferred:

1. **Compensation**: No salary information is mentioned, so salary_disclosed = false, all salary fields = null

2. **Work Authorization**: No mention of visa sponsorship, work authorization requirements, or citizenship restrictions. So visa_sponsorship_stated = "not_mentioned", work_auth_text = null, citizenship_text = null, security_clearance_stated = "not_mentioned"

3. **Work Model**: The location is "Hyderabad, Telangana, India" which suggests onsite work. No mention of remote or hybrid options. employment_type_stated = "full_time" (implied by the nature of the role). work_model_stated = "onsite", location_restriction_text = null

4. **Contract Details**: This appears to be a permanent full-time position, not a contract role. contract_type = "permanent", all other contract fields = null

5. **Contractor Rates**: Not applicable since this is not a contractor position. All fields = null

6. **Skills Classification**: Let me analyze the technical requirements:
   - Must-have hard skills: "Expertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter", "Knowledge of Cloud, Power BI, Data migration on cloud skills", "Experience in Unix shell scripting and python", "Experience with relational SQL, Big Data"
   - Nice-to-have hard skills: "Knowledge of MS-Azure Cloud", "Experience in Informatica PowerCenter" (this seems redundant), "Experience in Unix shell scripting and python" (also redundant)
   
   Wait, let me re-read this more carefully. The "Preferred Technical And Professional Experience" section lists:
   - Knowledge of MS-Azure Cloud
   - Experience in Informatica PowerCenter  
   - Experience in Unix shell scripting and python
   
   But these seem to overlap with the required skills. Let me be more careful about classification.
   
   The "Required Technical And Professional Expertise" section lists:
   - Expertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter
   - Knowledge of Cloud, Power BI, Data migration on cloud skills
   - Experience in Unix shell scripting and python
   - Experience with relational SQL, Big Data etc

   The "Preferred Technical And Professional Experience" section lists:
   - Knowledge of MS-Azure Cloud
   - Experience in Informatica PowerCenter
   - Experience in Unix shell scripting and python

   I notice some skills appear in both sections, which suggests they are critical requirements. The overlap between required and preferred skills indicates these are fundamental competencies for the role. The repeated skills like Informatica PowerCenter, Unix shell scripting, and Python highlight their importance in the job requirements.

   The soft skills section reveals a collaborative work environment emphasizing cross-functional teamwork. Key attributes include partnering with diverse professionals, bringing analytical rigor, and working in an Agile setting. The role demands strong communication and interpersonal abilities to navigate complex technical challenges across different teams.

   The position requires a Master's degree, with a focus on data engineering and integration. While the specific field isn't explicitly defined, the technical requirements suggest a background in computer science or a related technical discipline. The degree is marked as preferred, indicating some flexibility in educational qualifications.

   No specific certifications are mentioned, and the experience requirements are not precisely quantified. The role involves advanced technical skills in data management, cloud technologies, and programming languages like Python and Unix shell scripting.

   The position emphasizes expertise in data warehousing, information management, and business intelligence, with a strong focus on ETL tools and cloud-based data migration strategies. Proficiency in relational databases and big data technologies is also critical.

   Geographic constraints are clearly defined, targeting the Indian market with a specific focus on the Hyderabad region. The role appears to be a permanent, full-time position with no indication of remote work flexibility.

   The compensation details are not disclosed, suggesting a standard employment arrangement. No additional benefits or unique employment terms are highlighted in the job description.

   The company description provides context about IBM's consulting delivery centers, emphasizing their global reach and technical expertise across various sectors. These centers offer locally-based skills and technical capabilities to drive innovation and technology adoption for diverse clients.

   The role involves deep technical and industry expertise, working within IBM's Client Innovation Centers to deliver comprehensive solutions to public and private sector clients worldwide. The position focuses on data engineering, specifically data integration, highlighting the critical nature of data management in modern technological landscapes.

   The job requires extensive technical skills across multiple domains, including data warehousing, cloud technologies, and programming languages. Key competencies include expertise in ETL tools like Informatica PowerCenter, proficiency in Python and Unix shell scripting, and experience with big data technologies.

   The role demands a Master's degree, preferably in a technical field, with a strong emphasis on data engineering and integration capabilities. The position involves designing and implementing enterprise search applications, working collaboratively in Agile environments, and developing robust data pipelines for both batch and stream processing.

   The job requires expertise in data warehousing, information management, and business intelligence, with specific focus on ETL tools like Informatica PowerCenter. Candidates should have knowledge of cloud technologies, Power BI, and data migration strategies, along with programming skills in Python and Unix shell scripting.

   The preferred qualifications include experience with MS-Azure Cloud, Informatica PowerCenter, and advanced scripting capabilities. The role emphasizes collaborative work with data scientists and business analytics teams, requiring strong communication and problem-solving abilities.

   The position demands a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role requires deep technical expertise in data engineering and integration.

   The job involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The position demands expertise in data warehousing, information management, and business intelligence, with specific focus on ETL tools like Informatica PowerCenter. Candidates should have knowledge of cloud technologies, Power BI, and data migration strategies, along with programming skills in Python and Unix shell scripting.

   The preferred qualifications include experience with MS-Azure Cloud, Informatica PowerCenter, and advanced scripting capabilities. The role emphasizes collaborative work with data scientists and business analytics teams, requiring strong communication and problem-solving abilities.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like Elasticsearch and Splunk, working in Agile environments, and developing predictive models. Key responsibilities include building data pipelines, cleansing data, and collaborating with cross-functional teams to solve complex analytical challenges.

   The role emphasizes developing and maintaining data pipelines for both batch and stream processing, requiring close coordination with business teams and technical leads. Responsibilities include gathering requirements, identifying data sources, addressing data quality issues, and supporting user acceptance testing.

   The job requires a Master's degree, with a preference for technical fields like Computer Science or Data Science. While specific certifications aren't mentioned, the role demands deep technical expertise in data engineering and integration.

   The position involves designing enterprise search applications using tools like</reasoning>