<reasoning>I need to carefully extract information from this job posting following the strict rules provided. Let me go through each section systematically.

First, let me identify what information is explicitly stated in the job posting:

**Job Title**: Data Engineer-Data Platforms-AWS
**Company**: IBM
**Location**: Pune, Maharashtra, India

**Description sections**:
- Introduction about working in IBM Consulting Client Innovation Centers
- Role and Responsibilities as Data Engineer
- Education requirements
- Technical and Professional Expertise requirements

Let me extract information systematically:

**Compensation**: No salary information is mentioned, so all compensation fields should be null.

**Work Authorization**: No mention of visa sponsorship, work authorization requirements, or citizenship requirements. Should be "not_mentioned".

**Work Model**: The location is specified as "Pune, Maharashtra, India" but there's no explicit mention of remote/hybrid/onsite work model. The employment type isn't explicitly stated either. Should be "not_mentioned" for both.

**Contract Details**: No mention of contract type, duration, or any contract-related information. Should be "not_mentioned".

**Contractor Rates**: No contractor rates mentioned. Should be "not_mentioned".

**Skills Classification**: Let me analyze the technical requirements:

From the text:
- "Total 5-8+ years of experience in Data Management (DW, DL, Data Platform, Lakehouse) and Data Engineering skills"
- "Minimum 5+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala"
- "Minimum 3 years of experience on Cloud Data Platforms on AWS"
- "Experience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB"
- "Good to excellent SQL skills"
- "Apache Spark, Kafka, any Cloud computing etc"

For hard skills, I need to classify based on qualifiers:
- "Experienced in building data pipelines" - this is a responsibility, so should be must-have
- "Process the data with Spark, Python, PySpark, Scala, and Hive, Hbase or other NoSQL databases" - these are core requirements


- "Experience in AWS EMR / AWS Glue / DataBricks, AWS RedShift, DynamoDB" - these are specific requirements
- "Good to excellent SQL skills" - "good to excellent" is a strong qualifier, so must-have
- "Apache Spark, Kafka, any Cloud computing etc" - these are mentioned as part of the technology stack

I'll continue analyzing the technical requirements, focusing on the specific technologies and skills that demonstrate the candidate's expertise in data engineering and cloud platforms.

The preferred qualifications include a Master's Degree and AWS/DataBricks certification, which suggest advanced technical competency. The role demands deep experience in data management, spanning data warehousing, data lakes, and lakehouse architectures. Key technical requirements involve extensive big data technologies, particularly Spark, Python, and Scala, with a minimum of 5 years of hands-on experience.

The position requires significant cloud platform expertise, specifically 3+ years on AWS, and proficiency with critical AWS services like EMR, Glue, DataBricks, RedShift, and DynamoDB. Strong SQL skills are essential, indicating the need for robust data manipulation and analysis capabilities.

The role emphasizes comprehensive data engineering skills, including pipeline development, data processing across multiple technologies, and cloud-based data platform management. The candidate must demonstrate versatility in handling complex data transformation and integration challenges.

The position requires deep technical expertise in big data technologies, with a strong focus on Spark, Python, and Scala. Candidates need proven experience in cloud data platforms, particularly AWS, and must showcase advanced SQL capabilities. The role demands hands-on experience with multiple AWS services and data processing frameworks.

Key qualifications include extensive data engineering experience, cloud platform proficiency, and the ability to develop efficient software solutions using modern big data technologies. The ideal candidate will have a track record of implementing scalable data solutions across diverse technology stacks.

The role emphasizes comprehensive skills in data pipeline development, cloud computing, and data management, with a preference for advanced certifications and deep technical knowledge in distributed computing systems.

I'll focus on extracting key technical competencies across data engineering, cloud platforms, and programming languages. The position requires extensive experience in big data technologies, particularly Spark, Python, and Scala, with a strong emphasis on AWS cloud infrastructure.

The ideal candidate will demonstrate proficiency in data pipeline construction, cloud-based data processing, and have a robust background in managing complex data ecosystems across multiple platforms and technologies.

Key technical requirements include deep expertise in Spark, Python, and Scala, with significant experience in AWS cloud services like EMR, Glue, and DataBricks. The role demands strong SQL skills and a comprehensive understanding of data management across data warehouses, data lakes, and modern lakehouse architectures.

The position requires a minimum of 5-8 years in data management and engineering, with at least 5 years specifically in big data technologies. A Master's degree is preferred, along with AWS and DataBricks or Cloudera Spark certifications.

The role emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Key responsibilities include building scalable data solutions that can handle increasing data volumes.

The position demands expertise in streaming pipelines, Hadoop/AWS ecosystem components, and cloud computing technologies like Apache Spark and Kafka. Candidates should demonstrate strong technical skills in data engineering and cloud platforms.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should demonstrate expertise in streaming pipelines and scalable data solutions.

The role requires deep experience in data management, with a minimum of 5-8 years in data engineering. Candidates need extensive knowledge of big data technologies, particularly Spark, Python, or Scala, and at least 3 years of experience on AWS cloud data platforms.

Key technical requirements include proficiency in AWS EMR, AWS Glue, DataBricks, AWS RedShift, and DynamoDB. Strong SQL skills are essential, and certifications in AWS and DataBricks or Cloudera Spark are preferred qualifications.

The position emphasizes developing data pipelines, processing data across various platforms, and creating efficient software solutions using Spark Framework and cloud technologies. Candidates should</reasoning>