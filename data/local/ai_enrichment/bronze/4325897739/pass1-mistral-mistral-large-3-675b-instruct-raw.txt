```json
{
  "extraction": {
    "compensation": {
      "salary_disclosed": false,
      "salary_min": null,
      "salary_max": null,
      "salary_currency": null,
      "salary_period": null,
      "salary_text_raw": null,
      "hourly_rate_min": null,
      "hourly_rate_max": null,
      "hourly_rate_currency": null,
      "hourly_rate_text_raw": null
    },
    "work_authorization": {
      "visa_sponsorship_stated": "not_mentioned",
      "work_auth_text": null,
      "citizenship_text": null,
      "security_clearance_stated": "not_mentioned"
    },
    "work_model": {
      "work_model_stated": "not_mentioned",
      "location_restriction_text": null,
      "employment_type_stated": "full_time"
    },
    "contract_details": {
      "contract_type": "not_mentioned",
      "contract_duration_months": null,
      "contract_duration_text": null,
      "extension_possible": "not_mentioned",
      "conversion_to_fte": "not_mentioned",
      "start_date": "not_mentioned",
      "start_date_text": null,
      "probation_period_text": null
    },
    "contractor_rates": {
      "pay_type": "not_mentioned",
      "daily_rate_min": null,
      "daily_rate_max": null,
      "daily_rate_currency": null,
      "daily_rate_text_raw": null,
      "rate_negotiable": "not_mentioned",
      "overtime_paid": "not_mentioned"
    },
    "skills_classified": {
      "must_have_hard_skills": [
        "Spark Framework",
        "Python",
        "Scala",
        "Spark",
        "Hadoop",
        "Azure Cloud Data Platform",
        "HDFS",
        "PySpark",
        "Hive",
        "Hbase",
        "NoSQL databases",
        "SQL",
        "DataBricks",
        "Azure HDInsight",
        "Azure Data Factory",
        "Synapse",
        "SQL Server DB"
      ],
      "nice_to_have_hard_skills": [
        "Kafka"
      ],
      "must_have_soft_skills": [],
      "nice_to_have_soft_skills": [],
      "certifications_mentioned": [
        "Certification in Azure",
        "Data Bricks",
        "Cloudera Spark Certified"
      ],
      "years_experience_min": 4,
      "years_experience_max": null,
      "years_experience_text": "Minimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala",
      "education_level": null,
      "education_area": null,
      "education_requirement": "not_mentioned",
      "education_text_raw": null,
      "llm_genai_mentioned": false,
      "feature_store_mentioned": false
    },
    "geographic_restrictions": {
      "geo_restriction_type": "not_mentioned",
      "allowed_countries": null,
      "excluded_countries": null,
      "us_state_restrictions": null,
      "residency_requirement": "not_mentioned"
    },
    "benefits_structured": {
      "benefits_mentioned": [],
      "equity_mentioned": false,
      "learning_budget_mentioned": false,
      "conference_budget_mentioned": false,
      "hardware_choice_mentioned": false,
      "pto_policy": "not_mentioned"
    },
    "context": {
      "team_info_text": "In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.",
      "company_description_text": null
    }
  },
  "metadata": {
    "extraction_complete": true,
    "fields_found": 10,
    "fields_null": 26
  }
}
```