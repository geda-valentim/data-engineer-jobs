{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-Model Evaluation Analysis\n",
    "\n",
    "This notebook performs aggregate inter-model evaluation analysis across all jobs in `data/local/`.\n",
    "\n",
    "**Analysis Scope:**\n",
    "- Auto-discovers all jobs in data/local/\n",
    "- Aggregates metrics across all jobs\n",
    "- Provides detailed tables and visualizations\n",
    "- Focuses on model comparison, field types, pass trends, and low agreement fields\n",
    "\n",
    "**Generated:** " + "{{ datetime.now() }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluation modules\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.evaluation.evaluator import evaluate_job\n",
    "from src.evaluation.models import JobEvaluation, FieldEvaluation, ModelPerformance\n",
    "from src.evaluation.loader import MODEL_NAMES\n",
    "from src.evaluation.field_registry import PASS1_FIELDS, PASS2_FIELDS, PASS3_FIELDS\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"Analysis started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Discovery & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_and_evaluate_jobs(base_path=\"data/local\"):\n",
    "    \"\"\"\n",
    "    Auto-discover and evaluate all jobs in data/local directory.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (list of JobEvaluation objects, metadata dict)\n",
    "    \"\"\"\n",
    "    data_path = Path(base_path)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
    "    \n",
    "    # Discover job directories\n",
    "    job_dirs = [d for d in data_path.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "    job_ids = sorted([d.name for d in job_dirs])\n",
    "    \n",
    "    print(f\"Found {len(job_ids)} job(s): {job_ids}\")\n",
    "    \n",
    "    # Evaluate each job\n",
    "    job_evals = []\n",
    "    failed_jobs = []\n",
    "    \n",
    "    for job_id in job_ids:\n",
    "        try:\n",
    "            print(f\"Evaluating job {job_id}...\", end=\" \")\n",
    "            eval_result = evaluate_job(job_id, str(data_path))\n",
    "            job_evals.append(eval_result)\n",
    "            print(\"✓\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\")\n",
    "            failed_jobs.append((job_id, str(e)))\n",
    "    \n",
    "    metadata = {\n",
    "        'total_discovered': len(job_ids),\n",
    "        'successfully_evaluated': len(job_evals),\n",
    "        'failed': len(failed_jobs),\n",
    "        'job_ids': job_ids,\n",
    "        'failed_jobs': failed_jobs,\n",
    "        'evaluated_at': datetime.now()\n",
    "    }\n",
    "    \n",
    "    return job_evals, metadata\n",
    "\n",
    "# Execute discovery and evaluation\n",
    "job_evaluations, metadata = discover_and_evaluate_jobs()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Successfully evaluated: {metadata['successfully_evaluated']} job(s)\")\n",
    "print(f\"Failed: {metadata['failed']} job(s)\")\n",
    "if metadata['failed_jobs']:\n",
    "    print(f\"Failed jobs: {[job[0] for job in metadata['failed_jobs']]}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregate Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overall_metrics(job_evals):\n",
    "    \"\"\"\n",
    "    Compute overall consensus metrics across all jobs.\n",
    "    \"\"\"\n",
    "    if not job_evals:\n",
    "        return {}\n",
    "    \n",
    "    # Aggregate consensus rates\n",
    "    overall_rates = []\n",
    "    pass1_rates = []\n",
    "    pass2_rates = []\n",
    "    pass3_rates = []\n",
    "    total_fields = 0\n",
    "    \n",
    "    for job_eval in job_evals:\n",
    "        overall_rates.append(job_eval.overall_consensus_rate)\n",
    "        pass1_rates.append(job_eval.pass1_consensus_rate)\n",
    "        pass2_rates.append(job_eval.pass2_consensus_rate)\n",
    "        pass3_rates.append(job_eval.pass3_consensus_rate)\n",
    "        total_fields += job_eval.total_fields\n",
    "    \n",
    "    return {\n",
    "        'overall_consensus_rate': np.mean(overall_rates),\n",
    "        'pass1_consensus_rate': np.mean(pass1_rates),\n",
    "        'pass2_consensus_rate': np.mean(pass2_rates),\n",
    "        'pass3_consensus_rate': np.mean(pass3_rates),\n",
    "        'consensus_distribution': overall_rates,\n",
    "        'total_jobs': len(job_evals),\n",
    "        'total_fields': total_fields,\n",
    "        'avg_fields_per_job': total_fields / len(job_evals)\n",
    "    }\n",
    "\n",
    "overall_metrics = compute_overall_metrics(job_evaluations)\n",
    "print(\"✓ Overall metrics computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_field_type_metrics(job_evals):\n",
    "    \"\"\"\n",
    "    Compute field type performance metrics across all jobs.\n",
    "    \"\"\"\n",
    "    field_type_data = defaultdict(lambda: {'count': 0, 'with_consensus': 0, 'agreements': []})\n",
    "    \n",
    "    for job_eval in job_evals:\n",
    "        for field_eval in job_eval.field_evaluations:\n",
    "            ft = field_eval.field_type\n",
    "            field_type_data[ft]['count'] += 1\n",
    "            if field_eval.has_consensus:\n",
    "                field_type_data[ft]['with_consensus'] += 1\n",
    "            field_type_data[ft]['agreements'].append(field_eval.agreement_rate)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    for ft, data in field_type_data.items():\n",
    "        rows.append({\n",
    "            'field_type': ft,\n",
    "            'total_count': data['count'],\n",
    "            'with_consensus': data['with_consensus'],\n",
    "            'consensus_rate': data['with_consensus'] / data['count'] if data['count'] > 0 else 0,\n",
    "            'avg_agreement': np.mean(data['agreements']) if data['agreements'] else 0\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values('avg_agreement', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "field_type_metrics = compute_field_type_metrics(job_evaluations)\n",
    "print(\"✓ Field type metrics computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_performance_aggregate(job_evals):\n",
    "    \"\"\"\n",
    "    Compute aggregate model performance across all jobs.\n",
    "    \"\"\"\n",
    "    model_data = defaultdict(lambda: {\n",
    "        'agreements': [],\n",
    "        'pass1_agreements': [],\n",
    "        'pass2_agreements': [],\n",
    "        'pass3_agreements': [],\n",
    "        'outlier_rates': []\n",
    "    })\n",
    "    \n",
    "    for job_eval in job_evals:\n",
    "        for model_id, perf in job_eval.model_performance.items():\n",
    "            model_data[model_id]['agreements'].append(perf.agreement_rate)\n",
    "            model_data[model_id]['pass1_agreements'].append(perf.pass1_agreement)\n",
    "            model_data[model_id]['pass2_agreements'].append(perf.pass2_agreement)\n",
    "            model_data[model_id]['pass3_agreements'].append(perf.pass3_agreement)\n",
    "            model_data[model_id]['outlier_rates'].append(perf.outlier_rate)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    for model_id, data in model_data.items():\n",
    "        avg_agreement = np.mean(data['agreements'])\n",
    "        avg_outlier_rate = np.mean(data['outlier_rates'])\n",
    "        \n",
    "        # Calculate intermodel score (formula from report.py)\n",
    "        intermodel_score = (avg_agreement * 0.6 + (1 - avg_outlier_rate) * 0.4) * 100\n",
    "        \n",
    "        rows.append({\n",
    "            'model_id': model_id,\n",
    "            'model_name': MODEL_NAMES.get(model_id, model_id),\n",
    "            'avg_agreement': avg_agreement,\n",
    "            'pass1_agreement': np.mean(data['pass1_agreements']),\n",
    "            'pass2_agreement': np.mean(data['pass2_agreements']),\n",
    "            'pass3_agreement': np.mean(data['pass3_agreements']),\n",
    "            'outlier_rate': avg_outlier_rate,\n",
    "            'intermodel_score': intermodel_score\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values('intermodel_score', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "model_performance = compute_model_performance_aggregate(job_evaluations)\n",
    "print(\"✓ Model performance metrics computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pass_trends(job_evals):\n",
    "    \"\"\"\n",
    "    Compute pass-by-pass consensus trends for each job.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for job_eval in job_evals:\n",
    "        rows.append({\n",
    "            'job_id': job_eval.job_id,\n",
    "            'job_title': job_eval.job_title,\n",
    "            'company': job_eval.company,\n",
    "            'pass1_rate': job_eval.pass1_consensus_rate,\n",
    "            'pass2_rate': job_eval.pass2_consensus_rate,\n",
    "            'pass3_rate': job_eval.pass3_consensus_rate,\n",
    "            'overall_rate': job_eval.overall_consensus_rate\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "pass_trends = compute_pass_trends(job_evaluations)\n",
    "print(\"✓ Pass trends computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_low_agreement_fields(job_evals, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Identify fields with consistently low agreement across jobs.\n",
    "    \"\"\"\n",
    "    field_data = defaultdict(lambda: {'agreements': [], 'field_type': None, 'pass': None, 'outlier_models': []})\n",
    "    \n",
    "    for job_eval in job_evals:\n",
    "        for field_eval in job_eval.field_evaluations:\n",
    "            field_name = field_eval.field_name\n",
    "            field_data[field_name]['agreements'].append(field_eval.agreement_rate)\n",
    "            field_data[field_name]['field_type'] = field_eval.field_type\n",
    "            field_data[field_name]['pass'] = field_eval.pass_number\n",
    "            field_data[field_name]['outlier_models'].extend(field_eval.outliers)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    for field_name, data in field_data.items():\n",
    "        avg_agreement = np.mean(data['agreements'])\n",
    "        if avg_agreement < threshold:\n",
    "            outlier_counter = Counter(data['outlier_models'])\n",
    "            most_common_outlier = outlier_counter.most_common(1)[0][0] if outlier_counter else 'none'\n",
    "            \n",
    "            rows.append({\n",
    "                'field_name': field_name,\n",
    "                'field_type': data['field_type'],\n",
    "                'pass': data['pass'],\n",
    "                'avg_agreement': avg_agreement,\n",
    "                'appears_in_jobs': len(data['agreements']),\n",
    "                'most_common_outlier': most_common_outlier\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values('avg_agreement', ascending=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "low_agreement_fields = identify_low_agreement_fields(job_evaluations, threshold=0.6)\n",
    "print(f\"✓ Identified {len(low_agreement_fields)} low-agreement fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_high_agreement_fields(job_evals, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Identify fields with very high agreement across jobs.\n",
    "    \"\"\"\n",
    "    field_data = defaultdict(lambda: {'agreements': [], 'field_type': None, 'pass': None})\n",
    "    \n",
    "    for job_eval in job_evals:\n",
    "        for field_eval in job_eval.field_evaluations:\n",
    "            field_name = field_eval.field_name\n",
    "            field_data[field_name]['agreements'].append(field_eval.agreement_rate)\n",
    "            field_data[field_name]['field_type'] = field_eval.field_type\n",
    "            field_data[field_name]['pass'] = field_eval.pass_number\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    for field_name, data in field_data.items():\n",
    "        avg_agreement = np.mean(data['agreements'])\n",
    "        if avg_agreement >= threshold:\n",
    "            rows.append({\n",
    "                'field_name': field_name,\n",
    "                'field_type': data['field_type'],\n",
    "                'pass': data['pass'],\n",
    "                'avg_agreement': avg_agreement,\n",
    "                'appears_in_jobs': len(data['agreements'])\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values('avg_agreement', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "high_agreement_fields = identify_high_agreement_fields(job_evaluations, threshold=0.95)\n",
    "print(f\"✓ Identified {len(high_agreement_fields)} high-agreement fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_agreement_matrix(job_evals):\n",
    "    \"\"\"\n",
    "    Compute pairwise model agreement matrix.\n",
    "    \"\"\"\n",
    "    # Get all model IDs\n",
    "    if not job_evals:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    model_ids = list(job_evals[0].model_performance.keys())\n",
    "    \n",
    "    # Initialize agreement matrix\n",
    "    agreement_matrix = pd.DataFrame(0, index=model_ids, columns=model_ids, dtype=float)\n",
    "    \n",
    "    # Count pairwise agreements\n",
    "    for job_eval in job_evals:\n",
    "        for field_eval in job_eval.field_evaluations:\n",
    "            if not field_eval.has_consensus:\n",
    "                continue\n",
    "            \n",
    "            # Get models that agree with consensus\n",
    "            agreeing_models = [m for m in model_ids if m not in field_eval.outliers]\n",
    "            \n",
    "            # Increment pairwise agreement count\n",
    "            for i, model1 in enumerate(agreeing_models):\n",
    "                for model2 in agreeing_models[i:]:\n",
    "                    agreement_matrix.loc[model1, model2] += 1\n",
    "                    if model1 != model2:\n",
    "                        agreement_matrix.loc[model2, model1] += 1\n",
    "    \n",
    "    # Normalize to percentages\n",
    "    total_fields = sum(len(job_eval.field_evaluations) for job_eval in job_evals)\n",
    "    if total_fields > 0:\n",
    "        agreement_matrix = agreement_matrix / total_fields\n",
    "    \n",
    "    # Replace model IDs with names\n",
    "    model_names_list = [MODEL_NAMES.get(m, m) for m in model_ids]\n",
    "    agreement_matrix.index = model_names_list\n",
    "    agreement_matrix.columns = model_names_list\n",
    "    \n",
    "    return agreement_matrix\n",
    "\n",
    "model_agreement_matrix = compute_model_agreement_matrix(job_evaluations)\n",
    "print(\"✓ Model agreement matrix computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create executive summary\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Jobs Analyzed',\n",
    "        'Total Fields Evaluated',\n",
    "        'Avg Fields per Job',\n",
    "        'Overall Consensus Rate',\n",
    "        'Pass 1 Consensus Rate',\n",
    "        'Pass 2 Consensus Rate',\n",
    "        'Pass 3 Consensus Rate',\n",
    "        'Number of Models',\n",
    "        'Analysis Date'\n",
    "    ],\n",
    "    'Value': [\n",
    "        overall_metrics['total_jobs'],\n",
    "        overall_metrics['total_fields'],\n",
    "        f\"{overall_metrics['avg_fields_per_job']:.1f}\",\n",
    "        f\"{overall_metrics['overall_consensus_rate']:.1%}\",\n",
    "        f\"{overall_metrics['pass1_consensus_rate']:.1%}\",\n",
    "        f\"{overall_metrics['pass2_consensus_rate']:.1%}\",\n",
    "        f\"{overall_metrics['pass3_consensus_rate']:.1%}\",\n",
    "        len(model_performance),\n",
    "        datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Consensus by Field Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONSENSUS BY FIELD TYPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Format the DataFrame for display\n",
    "field_type_display = field_type_metrics.copy()\n",
    "field_type_display['consensus_rate'] = field_type_display['consensus_rate'].apply(lambda x: f\"{x:.1%}\")\n",
    "field_type_display['avg_agreement'] = field_type_display['avg_agreement'].apply(lambda x: f\"{x:.1%}\")\n",
    "\n",
    "print(field_type_display.to_string(index=False))\n",
    "\n",
    "# Styled version for notebooks that support it\n",
    "display(field_type_metrics.style\n",
    "    .format({\n",
    "        'consensus_rate': '{:.1%}',\n",
    "        'avg_agreement': '{:.1%}'\n",
    "    })\n",
    "    .background_gradient(subset=['avg_agreement'], cmap='RdYlGn', vmin=0, vmax=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model Performance Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE RANKINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Format the DataFrame for display\n",
    "model_display = model_performance[['model_name', 'avg_agreement', 'pass1_agreement', \n",
    "                                     'pass2_agreement', 'pass3_agreement', 'outlier_rate', \n",
    "                                     'intermodel_score']].copy()\n",
    "model_display.columns = ['Model', 'Avg Agreement', 'Pass 1', 'Pass 2', 'Pass 3', 'Outlier Rate', 'Score']\n",
    "\n",
    "for col in ['Avg Agreement', 'Pass 1', 'Pass 2', 'Pass 3', 'Outlier Rate']:\n",
    "    model_display[col] = model_display[col].apply(lambda x: f\"{x:.1%}\")\n",
    "model_display['Score'] = model_display['Score'].apply(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "print(model_display.to_string(index=False))\n",
    "\n",
    "# Styled version\n",
    "display(model_performance[['model_name', 'avg_agreement', 'pass1_agreement', \n",
    "                            'pass2_agreement', 'pass3_agreement', 'outlier_rate', \n",
    "                            'intermodel_score']].style\n",
    "    .format({\n",
    "        'avg_agreement': '{:.1%}',\n",
    "        'pass1_agreement': '{:.1%}',\n",
    "        'pass2_agreement': '{:.1%}',\n",
    "        'pass3_agreement': '{:.1%}',\n",
    "        'outlier_rate': '{:.1%}',\n",
    "        'intermodel_score': '{:.1f}'\n",
    "    })\n",
    "    .background_gradient(subset=['intermodel_score'], cmap='RdYlGn')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Low-Agreement Fields (Gap Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"LOW-AGREEMENT FIELDS (< 60%)\")\n",
    "print(f\"Total: {len(low_agreement_fields)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not low_agreement_fields.empty:\n",
    "    # Show top 20\n",
    "    display_df = low_agreement_fields.head(20).copy()\n",
    "    display_df['avg_agreement'] = display_df['avg_agreement'].apply(lambda x: f\"{x:.1%}\")\n",
    "    print(display_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No low-agreement fields found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(model_performance))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - width*1.5, model_performance['avg_agreement'], width, label='Overall', alpha=0.8)\n",
    "ax.bar(x - width*0.5, model_performance['pass1_agreement'], width, label='Pass 1', alpha=0.8)\n",
    "ax.bar(x + width*0.5, model_performance['pass2_agreement'], width, label='Pass 2', alpha=0.8)\n",
    "ax.bar(x + width*1.5, model_performance['pass3_agreement'], width, label='Pass 3', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Agreement Rate', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison by Pass', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_performance['model_name'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (_, row) in enumerate(model_performance.iterrows()):\n",
    "    ax.text(i - width*1.5, row['avg_agreement'] + 0.02, f\"{row['avg_agreement']:.0%}\", \n",
    "            ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Field Type Agreement Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build field type x model heatmap\n",
    "field_type_model_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for job_eval in job_evaluations:\n",
    "    for field_eval in job_eval.field_evaluations:\n",
    "        ft = field_eval.field_type\n",
    "        for model_id in job_eval.model_performance.keys():\n",
    "            # Check if model agrees with consensus\n",
    "            is_outlier = model_id in field_eval.outliers\n",
    "            field_type_model_data[ft][model_id].append(0 if is_outlier else 1)\n",
    "\n",
    "# Convert to DataFrame\n",
    "heatmap_data = []\n",
    "for ft in field_type_model_data.keys():\n",
    "    row = {'field_type': ft}\n",
    "    for model_id in model_performance['model_id']:\n",
    "        model_name = MODEL_NAMES.get(model_id, model_id)\n",
    "        agreements = field_type_model_data[ft][model_id]\n",
    "        row[model_name] = np.mean(agreements) if agreements else 0\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "heatmap_df = pd.DataFrame(heatmap_data)\n",
    "heatmap_df = heatmap_df.set_index('field_type')\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_df, annot=True, fmt='.0%', cmap='RdYlGn', vmin=0, vmax=1, \n",
    "            cbar_kws={'label': 'Agreement Rate'})\n",
    "plt.title('Field Type Agreement by Model', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Field Type', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Pass-by-Pass Consensus Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = range(len(pass_trends))\n",
    "ax.plot(x, pass_trends['pass1_rate'], marker='o', label='Pass 1 (Extraction)', linewidth=2)\n",
    "ax.plot(x, pass_trends['pass2_rate'], marker='s', label='Pass 2 (Inference)', linewidth=2)\n",
    "ax.plot(x, pass_trends['pass3_rate'], marker='^', label='Pass 3 (Analysis)', linewidth=2)\n",
    "ax.plot(x, pass_trends['overall_rate'], marker='D', label='Overall', linewidth=2, linestyle='--', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Job', fontsize=12)\n",
    "ax.set_ylabel('Consensus Rate', fontsize=12)\n",
    "ax.set_title('Pass-by-Pass Consensus Trends Across Jobs', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{row['job_id'][:8]}...\" for _, row in pass_trends.iterrows()], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Consensus Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "consensus_dist = overall_metrics['consensus_distribution']\n",
    "ax.hist(consensus_dist, bins=15, alpha=0.7, edgecolor='black', density=True)\n",
    "\n",
    "# Add KDE\n",
    "from scipy import stats\n",
    "if len(consensus_dist) > 1:\n",
    "    kde = stats.gaussian_kde(consensus_dist)\n",
    "    x_range = np.linspace(min(consensus_dist), max(consensus_dist), 100)\n",
    "    ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "\n",
    "# Add mean and median lines\n",
    "mean_val = np.mean(consensus_dist)\n",
    "median_val = np.median(consensus_dist)\n",
    "ax.axvline(mean_val, color='blue', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.1%}')\n",
    "ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.1%}')\n",
    "\n",
    "ax.set_xlabel('Overall Consensus Rate', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Distribution of Overall Consensus Rates Across Jobs', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Top Low-Agreement Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not low_agreement_fields.empty:\n",
    "    # Show top 20 worst fields\n",
    "    top_low = low_agreement_fields.head(20).copy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Create color map by field type\n",
    "    field_types = top_low['field_type'].unique()\n",
    "    colors = sns.color_palette('husl', len(field_types))\n",
    "    color_map = dict(zip(field_types, colors))\n",
    "    bar_colors = [color_map[ft] for ft in top_low['field_type']]\n",
    "    \n",
    "    y_pos = range(len(top_low))\n",
    "    ax.barh(y_pos, top_low['avg_agreement'], color=bar_colors, alpha=0.7)\n",
    "    \n",
    "    # Truncate field names for display\n",
    "    field_labels = [f[:40] + '...' if len(f) > 40 else f for f in top_low['field_name']]\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(field_labels, fontsize=9)\n",
    "    ax.set_xlabel('Agreement Rate', fontsize=12)\n",
    "    ax.set_title('Top 20 Low-Agreement Fields', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add legend for field types\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color_map[ft], label=ft, alpha=0.7) for ft in field_types]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', title='Field Type')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No low-agreement fields to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Model Pairwise Agreement Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_agreement_matrix.empty:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(model_agreement_matrix, annot=True, fmt='.0%', cmap='YlGnBu', \n",
    "                square=True, cbar_kws={'label': 'Agreement Rate'})\n",
    "    plt.title('Model Pairwise Agreement Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Model', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot compute model agreement matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Field Type Agreement Distribution (Box Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect agreement rates by field type\n",
    "field_type_distributions = defaultdict(list)\n",
    "\n",
    "for job_eval in job_evaluations:\n",
    "    for field_eval in job_eval.field_evaluations:\n",
    "        field_type_distributions[field_eval.field_type].append(field_eval.agreement_rate)\n",
    "\n",
    "# Convert to long-form DataFrame for seaborn\n",
    "box_data = []\n",
    "for ft, agreements in field_type_distributions.items():\n",
    "    for agreement in agreements:\n",
    "        box_data.append({'field_type': ft, 'agreement_rate': agreement})\n",
    "\n",
    "box_df = pd.DataFrame(box_data)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.boxplot(data=box_df, x='field_type', y='agreement_rate', ax=ax)\n",
    "sns.stripplot(data=box_df, x='field_type', y='agreement_rate', ax=ax, \n",
    "              color='black', alpha=0.3, size=2)\n",
    "\n",
    "ax.set_xlabel('Field Type', fontsize=12)\n",
    "ax.set_ylabel('Agreement Rate', fontsize=12)\n",
    "ax.set_title('Agreement Rate Distribution by Field Type', fontsize=14, fontweight='bold')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Job-by-Job Consensus Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = range(len(pass_trends))\n",
    "colors = plt.cm.viridis(pass_trends['overall_rate'])\n",
    "\n",
    "bars = ax.bar(x, pass_trends['overall_rate'], color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Job', fontsize=12)\n",
    "ax.set_ylabel('Overall Consensus Rate', fontsize=12)\n",
    "ax.set_title('Overall Consensus Rate by Job', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{row['job_id'][:8]}...\\n{row['company'][:15]}\" \n",
    "                     for _, row in pass_trends.iterrows()], rotation=45, ha='right', fontsize=9)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (_, row) in enumerate(pass_trends.iterrows()):\n",
    "    ax.text(i, row['overall_rate'] + 0.02, f\"{row['overall_rate']:.0%}\", \n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=0, vmax=1))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Consensus Rate', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Drill-Down Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 All Fields Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive field table\n",
    "all_fields_data = []\n",
    "\n",
    "for job_eval in job_evaluations:\n",
    "    for field_eval in job_eval.field_evaluations:\n",
    "        all_fields_data.append({\n",
    "            'job_id': job_eval.job_id,\n",
    "            'company': job_eval.company,\n",
    "            'field_name': field_eval.field_name,\n",
    "            'field_type': field_eval.field_type,\n",
    "            'pass': field_eval.pass_number,\n",
    "            'has_consensus': field_eval.has_consensus,\n",
    "            'agreement_rate': field_eval.agreement_rate,\n",
    "            'outliers': ', '.join(field_eval.outliers) if field_eval.outliers else 'none'\n",
    "        })\n",
    "\n",
    "all_fields_df = pd.DataFrame(all_fields_data)\n",
    "\n",
    "print(f\"\\nTotal field evaluations: {len(all_fields_df)}\")\n",
    "print(\"\\nSample of all fields (first 20):\")\n",
    "display(all_fields_df.head(20))\n",
    "\n",
    "# Can be filtered/sorted interactively\n",
    "print(\"\\nThis table contains all field evaluations and can be filtered/sorted as needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze outlier patterns by model\n",
    "outlier_data = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for job_eval in job_evaluations:\n",
    "    for field_eval in job_eval.field_evaluations:\n",
    "        for outlier_model in field_eval.outliers:\n",
    "            key = f\"{field_eval.field_type}|Pass{field_eval.pass_number}\"\n",
    "            outlier_data[outlier_model][key] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "outlier_rows = []\n",
    "for model_id, field_type_counts in outlier_data.items():\n",
    "    for key, count in field_type_counts.items():\n",
    "        field_type, pass_num = key.split('|')\n",
    "        outlier_rows.append({\n",
    "            'model': MODEL_NAMES.get(model_id, model_id),\n",
    "            'field_type': field_type,\n",
    "            'pass': pass_num,\n",
    "            'outlier_count': count\n",
    "        })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_rows)\n",
    "if not outlier_df.empty:\n",
    "    outlier_df = outlier_df.sort_values(['model', 'outlier_count'], ascending=[True, False])\n",
    "\n",
    "print(\"\\nMODEL OUTLIER ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Shows how often each model is an outlier by field type and pass\")\n",
    "print(\"=\"*60)\n",
    "display(outlier_df.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Per-Job Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the pass_trends DataFrame which has per-job summary\n",
    "print(\"\\nPER-JOB EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "job_summary = pass_trends.copy()\n",
    "for col in ['pass1_rate', 'pass2_rate', 'pass3_rate', 'overall_rate']:\n",
    "    job_summary[f\"{col}_pct\"] = job_summary[col].apply(lambda x: f\"{x:.1%}\")\n",
    "\n",
    "display_cols = ['job_id', 'job_title', 'company', 'overall_rate_pct', \n",
    "                'pass1_rate_pct', 'pass2_rate_pct', 'pass3_rate_pct']\n",
    "display(job_summary[display_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "print(\"\\nCORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Correlations between different metrics\n",
    "correlations = []\n",
    "\n",
    "# Overall vs Pass rates\n",
    "for pass_col in ['pass1_rate', 'pass2_rate', 'pass3_rate']:\n",
    "    r, p = pearsonr(pass_trends['overall_rate'], pass_trends[pass_col])\n",
    "    correlations.append({\n",
    "        'metric_1': 'overall_rate',\n",
    "        'metric_2': pass_col,\n",
    "        'pearson_r': r,\n",
    "        'p_value': p,\n",
    "        'significant': 'Yes' if p < 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "# Agreement vs Outlier rate (from model performance)\n",
    "if len(model_performance) > 1:\n",
    "    r, p = pearsonr(model_performance['avg_agreement'], model_performance['outlier_rate'])\n",
    "    correlations.append({\n",
    "        'metric_1': 'agreement_rate',\n",
    "        'metric_2': 'outlier_rate',\n",
    "        'pearson_r': r,\n",
    "        'p_value': p,\n",
    "        'significant': 'Yes' if p < 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "corr_df = pd.DataFrame(correlations)\n",
    "display(corr_df)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Strong positive correlation (r > 0.7): Variables move together\")\n",
    "print(\"- Strong negative correlation (r < -0.7): Variables move opposite\")\n",
    "print(\"- Weak correlation (-0.3 < r < 0.3): Little linear relationship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Field Type Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFIELD TYPE IMPACT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Which field types most impact overall consensus?\\n\")\n",
    "\n",
    "# Calculate weighted impact (count * avg_agreement)\n",
    "impact_df = field_type_metrics.copy()\n",
    "impact_df['impact_score'] = impact_df['total_count'] * impact_df['avg_agreement']\n",
    "impact_df['weight'] = impact_df['total_count'] / impact_df['total_count'].sum()\n",
    "\n",
    "impact_df = impact_df.sort_values('impact_score', ascending=False)\n",
    "\n",
    "display(impact_df[['field_type', 'total_count', 'avg_agreement', 'weight', 'impact_score']])\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Impact score = count × avg_agreement\")\n",
    "print(\"- Higher impact score means field type has more influence on overall consensus\")\n",
    "print(\"- Weight shows proportion of total fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Model Specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMODEL SPECIALIZATION BY FIELD TYPE\")\n",
    "print(\"=\"*60)\n",
    "print(\"Which models excel at which field types?\\n\")\n",
    "\n",
    "# Use the heatmap data from visualization 5.2\n",
    "if not heatmap_df.empty:\n",
    "    # Find best model for each field type\n",
    "    best_models = []\n",
    "    for ft in heatmap_df.index:\n",
    "        best_model = heatmap_df.loc[ft].idxmax()\n",
    "        best_score = heatmap_df.loc[ft].max()\n",
    "        best_models.append({\n",
    "            'field_type': ft,\n",
    "            'best_model': best_model,\n",
    "            'agreement_rate': best_score\n",
    "        })\n",
    "    \n",
    "    specialization_df = pd.DataFrame(best_models)\n",
    "    specialization_df['agreement_rate'] = specialization_df['agreement_rate'].apply(lambda x: f\"{x:.1%}\")\n",
    "    \n",
    "    display(specialization_df)\n",
    "    \n",
    "    print(\"\\nModel Expertise Summary:\")\n",
    "    expertise_count = specialization_df['best_model'].value_counts()\n",
    "    for model, count in expertise_count.items():\n",
    "        print(f\"  {model}: Best at {count} field type(s)\")\n",
    "else:\n",
    "    print(\"Insufficient data for specialization analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export & Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Export Key Tables to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exports directory if it doesn't exist\n",
    "export_dir = Path('exports')\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Export key tables\n",
    "exports = {\n",
    "    'model_performance': model_performance,\n",
    "    'field_type_metrics': field_type_metrics,\n",
    "    'low_agreement_fields': low_agreement_fields,\n",
    "    'high_agreement_fields': high_agreement_fields,\n",
    "    'pass_trends': pass_trends,\n",
    "    'all_fields': all_fields_df\n",
    "}\n",
    "\n",
    "print(\"Exporting tables to CSV...\")\n",
    "for name, df in exports.items():\n",
    "    if not df.empty:\n",
    "        filepath = export_dir / f\"{name}_{timestamp}.csv\"\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"  ✓ Exported: {filepath}\")\n",
    "\n",
    "print(f\"\\nAll exports saved to: {export_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Generate Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown summary\n",
    "best_model = model_performance.iloc[0]['model_name']\n",
    "worst_model = model_performance.iloc[-1]['model_name']\n",
    "best_field_type = field_type_metrics.iloc[0]['field_type']\n",
    "worst_field_type = field_type_metrics.iloc[-1]['field_type']\n",
    "\n",
    "summary_md = f\"\"\"\n",
    "# Inter-Model Evaluation Analysis Summary\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Overview\n",
    "- **Jobs Analyzed:** {overall_metrics['total_jobs']}\n",
    "- **Total Fields Evaluated:** {overall_metrics['total_fields']}\n",
    "- **Overall Consensus Rate:** {overall_metrics['overall_consensus_rate']:.1%}\n",
    "\n",
    "## Pass-by-Pass Performance\n",
    "- **Pass 1 (Extraction):** {overall_metrics['pass1_consensus_rate']:.1%}\n",
    "- **Pass 2 (Inference):** {overall_metrics['pass2_consensus_rate']:.1%}\n",
    "- **Pass 3 (Analysis):** {overall_metrics['pass3_consensus_rate']:.1%}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Model Performance\n",
    "- **Best Performer:** {best_model} (Score: {model_performance.iloc[0]['intermodel_score']:.1f})\n",
    "- **Needs Improvement:** {worst_model} (Score: {model_performance.iloc[-1]['intermodel_score']:.1f})\n",
    "\n",
    "### Field Type Analysis\n",
    "- **Highest Agreement:** {best_field_type} ({field_type_metrics.iloc[0]['avg_agreement']:.1%})\n",
    "- **Lowest Agreement:** {worst_field_type} ({field_type_metrics.iloc[-1]['avg_agreement']:.1%})\n",
    "- **Gap Fields:** {len(low_agreement_fields)} fields with <60% agreement\n",
    "\n",
    "### Recommendations\n",
    "1. **Focus on improving:** {worst_field_type} field types (lowest agreement)\n",
    "2. **Review models:** Investigate why {worst_model} has lower performance\n",
    "3. **Address gaps:** Review {len(low_agreement_fields)} low-agreement fields for prompt improvements\n",
    "4. **Leverage strengths:** {best_model} shows strong performance, consider as reference\n",
    "\n",
    "## Next Steps\n",
    "- Review detailed field-level analysis for specific improvements\n",
    "- Consider adding more models or adjusting prompts for low-agreement fields\n",
    "- Monitor trends as new jobs are added\n",
    "\"\"\"\n",
    "\n",
    "# Save summary\n",
    "summary_path = export_dir / f\"executive_summary_{timestamp}.md\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(summary_md)\n",
    "\n",
    "print(f\"Executive summary saved to: {summary_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(summary_md)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Insight 1: Overall performance\n",
    "if overall_metrics['overall_consensus_rate'] >= 0.8:\n",
    "    insights.append(f\"✓ Strong overall consensus ({overall_metrics['overall_consensus_rate']:.1%}) indicates good model alignment\")\n",
    "elif overall_metrics['overall_consensus_rate'] >= 0.6:\n",
    "    insights.append(f\"⚠ Moderate consensus ({overall_metrics['overall_consensus_rate']:.1%}) - room for improvement\")\n",
    "else:\n",
    "    insights.append(f\"✗ Low consensus ({overall_metrics['overall_consensus_rate']:.1%}) - significant gaps to address\")\n",
    "\n",
    "# Insight 2: Pass variation\n",
    "pass_rates = [overall_metrics['pass1_consensus_rate'], \n",
    "              overall_metrics['pass2_consensus_rate'], \n",
    "              overall_metrics['pass3_consensus_rate']]\n",
    "pass_std = np.std(pass_rates)\n",
    "if pass_std > 0.15:\n",
    "    insights.append(f\"⚠ High variation between passes (σ={pass_std:.2f}) - some passes need attention\")\n",
    "else:\n",
    "    insights.append(f\"✓ Consistent performance across passes (σ={pass_std:.2f})\")\n",
    "\n",
    "# Insight 3: Model spread\n",
    "if len(model_performance) > 1:\n",
    "    score_spread = model_performance['intermodel_score'].max() - model_performance['intermodel_score'].min()\n",
    "    if score_spread > 10:\n",
    "        insights.append(f\"⚠ Large spread in model scores ({score_spread:.1f} points) - consider model selection\")\n",
    "    else:\n",
    "        insights.append(f\"✓ Models perform similarly (spread: {score_spread:.1f} points)\")\n",
    "\n",
    "# Insight 4: Gap fields\n",
    "if len(low_agreement_fields) > 0:\n",
    "    gap_pct = len(low_agreement_fields) / overall_metrics['total_fields'] * 100\n",
    "    insights.append(f\"⚠ {len(low_agreement_fields)} gap fields ({gap_pct:.1f}% of total) need review\")\n",
    "else:\n",
    "    insights.append(\"✓ No significant gap fields - strong alignment across all fields\")\n",
    "\n",
    "# Insight 5: Field type challenges\n",
    "if not field_type_metrics.empty:\n",
    "    worst_ft = field_type_metrics.iloc[-1]\n",
    "    if worst_ft['avg_agreement'] < 0.6:\n",
    "        insights.append(f\"⚠ {worst_ft['field_type']} fields are challenging ({worst_ft['avg_agreement']:.1%} agreement)\")\n",
    "\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Analysis completed at: {datetime.now()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Analysis Complete!\n",
    "\n",
    "This notebook has provided:\n",
    "- Aggregate metrics across all jobs\n",
    "- Model performance comparisons\n",
    "- Field type analysis\n",
    "- 8 comprehensive visualizations\n",
    "- Detailed drill-down tables\n",
    "- Statistical insights\n",
    "- Exported results to CSV\n",
    "\n",
    "**Next time you run this notebook:**\n",
    "- It will automatically discover any new jobs added to `data/local/`\n",
    "- All metrics and visualizations will update accordingly\n",
    "- New exports will be timestamped to avoid overwriting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
